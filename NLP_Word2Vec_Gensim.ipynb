{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = corpora.Dictionary(texts)\n",
    "corpus = [dct.doc2bow(line) for line in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1)],\n",
       " [(9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1)],\n",
       " [(7, 2),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1)],\n",
       " [(23, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
     ]
    }
   ],
   "source": [
    "# Show the word to id map\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding to an existing dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "\n",
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(48 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32, 'graph': 33, 'in': 34, 'intersection': 35, 'paths': 36, 'trees': 37, 'Graph': 38, 'IV': 39, 'Widths': 40, 'and': 41, 'minors': 42, 'ordering': 43, 'quasi': 44, 'well': 45, 'A': 46, 'survey': 47}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The think tank of China’s People’s Liberation Army Rocket Force recently recruited 13 Chinese technicians\n",
      "\n",
      " from private companies, PLA Daily reported on Saturday.\n",
      "\n",
      "Zhang Hao and 12 other science and technology experts received letters of appointment at the founding ceremony of\n",
      "\n",
      "the PLA Rocket Force national defense science and technology experts panel, according to a report published by the\n",
      "\n",
      "PLA Daily on Saturday.\n",
      "\n",
      "Honored as “rocket force science and technology experts,” Zhang and his fellow experts from private companies will\n",
      "\n",
      "serve as members of the PLA Rocket Force think tank, which will conduct research into fields like overall design of\n",
      "\n",
      "the missiles, missile launching and network system technology for five years.\n",
      "\n",
      "The experts will enjoy the same treatment as their counterparts from State-owned firms, the report said.\n",
      "\n",
      "The PLA Daily said that this marks a new development in deepening military-civilian integration in China, which\n",
      "\n",
      "could make science and technology innovation better contribute to the enhancement of the force’s combat capabilities.\n"
     ]
    }
   ],
   "source": [
    "# Create gensim dictionary form a single tet file\n",
    "dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('gensim_sample.txt'))\n",
    "for line in open('gensim_sample.txt'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key      Label           Number    \n",
      "0        0               army      \n",
      "1        1               china     \n",
      "2        2               chinese   \n",
      "3        3               force     \n",
      "4        4               liberation\n",
      "5        5               of        \n",
      "6        6               people    \n",
      "7        7               recently  \n",
      "8        8               recruited \n",
      "9        9               rocket    \n",
      "10       10              tank      \n",
      "11       11              technicians\n",
      "12       12              the       \n",
      "13       13              think     \n",
      "14       14              companies \n",
      "15       15              daily     \n",
      "16       16              from      \n",
      "17       17              on        \n",
      "18       18              pla       \n",
      "19       19              private   \n",
      "20       20              reported  \n",
      "21       21              saturday  \n",
      "22       22              and       \n",
      "23       23              appointment\n",
      "24       24              at        \n",
      "25       25              ceremony  \n",
      "26       26              experts   \n",
      "27       27              founding  \n",
      "28       28              hao       \n",
      "29       29              letters   \n",
      "30       30              other     \n",
      "31       31              received  \n",
      "32       32              science   \n",
      "33       33              technology\n",
      "34       34              zhang     \n",
      "35       35              according \n",
      "36       36              by        \n",
      "37       37              defense   \n",
      "38       38              national  \n",
      "39       39              panel     \n",
      "40       40              published \n",
      "41       41              report    \n",
      "42       42              to        \n",
      "43       43              as        \n",
      "44       44              fellow    \n",
      "45       45              his       \n",
      "46       46              honored   \n",
      "47       47              will      \n",
      "48       48              conduct   \n",
      "49       49              design    \n",
      "50       50              fields    \n",
      "51       51              into      \n",
      "52       52              like      \n",
      "53       53              members   \n",
      "54       54              overall   \n",
      "55       55              research  \n",
      "56       56              serve     \n",
      "57       57              which     \n",
      "58       58              five      \n",
      "59       59              for       \n",
      "60       60              launching \n",
      "61       61              missile   \n",
      "62       62              missiles  \n",
      "63       63              network   \n",
      "64       64              system    \n",
      "65       65              years     \n",
      "66       66              counterparts\n",
      "67       67              enjoy     \n",
      "68       68              firms     \n",
      "69       69              owned     \n",
      "70       70              said      \n",
      "71       71              same      \n",
      "72       72              state     \n",
      "73       73              their     \n",
      "74       74              treatment \n",
      "75       75              civilian  \n",
      "76       76              deepening \n",
      "77       77              development\n",
      "78       78              in        \n",
      "79       79              integration\n",
      "80       80              marks     \n",
      "81       81              military  \n",
      "82       82              new       \n",
      "83       83              that      \n",
      "84       84              this      \n",
      "85       85              better    \n",
      "86       86              capabilities\n",
      "87       87              combat    \n",
      "88       88              contribute\n",
      "89       89              could     \n",
      "90       90              enhancement\n",
      "91       91              innovation\n",
      "92       92              make      \n"
     ]
    }
   ],
   "source": [
    "print (\"{:<8} {:<15} {:<10}\".format('Key','Label','Number'))\n",
    "for k, v in dictionary.iteritems():\n",
    "#     label, num = v\n",
    "    print (\"{:<8} {:<15} {:<10}\".format(k, k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'according': 35,\n",
       " 'and': 22,\n",
       " 'appointment': 23,\n",
       " 'army': 0,\n",
       " 'as': 43,\n",
       " 'at': 24,\n",
       " 'better': 85,\n",
       " 'by': 36,\n",
       " 'capabilities': 86,\n",
       " 'ceremony': 25,\n",
       " 'china': 1,\n",
       " 'chinese': 2,\n",
       " 'civilian': 75,\n",
       " 'combat': 87,\n",
       " 'companies': 14,\n",
       " 'conduct': 48,\n",
       " 'contribute': 88,\n",
       " 'could': 89,\n",
       " 'counterparts': 66,\n",
       " 'daily': 15,\n",
       " 'deepening': 76,\n",
       " 'defense': 37,\n",
       " 'design': 49,\n",
       " 'development': 77,\n",
       " 'enhancement': 90,\n",
       " 'enjoy': 67,\n",
       " 'experts': 26,\n",
       " 'fellow': 44,\n",
       " 'fields': 50,\n",
       " 'firms': 68,\n",
       " 'five': 58,\n",
       " 'for': 59,\n",
       " 'force': 3,\n",
       " 'founding': 27,\n",
       " 'from': 16,\n",
       " 'hao': 28,\n",
       " 'his': 45,\n",
       " 'honored': 46,\n",
       " 'in': 78,\n",
       " 'innovation': 91,\n",
       " 'integration': 79,\n",
       " 'into': 51,\n",
       " 'launching': 60,\n",
       " 'letters': 29,\n",
       " 'liberation': 4,\n",
       " 'like': 52,\n",
       " 'make': 92,\n",
       " 'marks': 80,\n",
       " 'members': 53,\n",
       " 'military': 81,\n",
       " 'missile': 61,\n",
       " 'missiles': 62,\n",
       " 'national': 38,\n",
       " 'network': 63,\n",
       " 'new': 82,\n",
       " 'of': 5,\n",
       " 'on': 17,\n",
       " 'other': 30,\n",
       " 'overall': 54,\n",
       " 'owned': 69,\n",
       " 'panel': 39,\n",
       " 'people': 6,\n",
       " 'pla': 18,\n",
       " 'private': 19,\n",
       " 'published': 40,\n",
       " 'received': 31,\n",
       " 'recently': 7,\n",
       " 'recruited': 8,\n",
       " 'report': 41,\n",
       " 'reported': 20,\n",
       " 'research': 55,\n",
       " 'rocket': 9,\n",
       " 'said': 70,\n",
       " 'same': 71,\n",
       " 'saturday': 21,\n",
       " 'science': 32,\n",
       " 'serve': 56,\n",
       " 'state': 72,\n",
       " 'system': 64,\n",
       " 'tank': 10,\n",
       " 'technicians': 11,\n",
       " 'technology': 33,\n",
       " 'that': 83,\n",
       " 'the': 12,\n",
       " 'their': 73,\n",
       " 'think': 13,\n",
       " 'this': 84,\n",
       " 'to': 42,\n",
       " 'treatment': 74,\n",
       " 'which': 57,\n",
       " 'will': 47,\n",
       " 'years': 65,\n",
       " 'zhang': 34}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token to Id map\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List with 2 sentences\n",
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "my_other_docs = [\"India is my country\",\"All Indians are\",\"my brothers and sisters\"]\n",
    "# Tokenize the docs\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
    "tokenized_list1 = [simple_preprocess(doc) for doc in my_other_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['india', 'is', 'my', 'country'],\n",
       "  ['all', 'indians', 'are'],\n",
       "  ['my', 'brothers', 'and', 'sisters']],\n",
       " [['who', 'let', 'the', 'dogs', 'out'], ['who', 'who', 'who', 'who']])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_list1, tokenized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(10, 1), (11, 1), (12, 1), (13, 1), (14, 1)], [(14, 4)]]\n"
     ]
    }
   ],
   "source": [
    "# Create the Corpus\n",
    "mydict2 = corpora.Dictionary(tokenized_list1)\n",
    "mycorpus = [mydict2.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "pprint(mycorpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 country\n",
      "1 india\n",
      "2 is\n",
      "3 my\n",
      "4 all\n",
      "5 are\n",
      "6 indians\n",
      "7 and\n",
      "8 brothers\n",
      "9 sisters\n"
     ]
    }
   ],
   "source": [
    "for k, v in mydict2.iteritems():\n",
    "    print(k , v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(10, 1), (11, 1), (12, 1), (13, 1), (14, 1)], [(14, 4)]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = [[(mydict2[id], count) for id, count in line] for line in mycorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('dogs', 1), ('let', 1), ('out', 1), ('the', 1), ('who', 1)], [('who', 4)]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Corpus / Dictionary in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the Dict and Corpus\n",
    "mydict2.save('mydict.dict')  # save dict to disk\n",
    "corpora.MmCorpus.serialize('bow_corpus.mm', mycorpus)  # save corpus to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 country\n",
      "1 india\n",
      "2 is\n",
      "3 my\n",
      "4 all\n",
      "5 are\n",
      "6 indians\n",
      "7 and\n",
      "8 brothers\n",
      "9 sisters\n",
      "10 dogs\n",
      "11 let\n",
      "12 out\n",
      "13 the\n",
      "14 who\n"
     ]
    }
   ],
   "source": [
    "# Load them back\n",
    "loaded_dict = corpora.Dictionary.load('mydict.dict')\n",
    "for k, v in loaded_dict.iteritems():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0)]\n",
      "[(14, 4.0)]\n"
     ]
    }
   ],
   "source": [
    "corpus = corpora.MmCorpus('bow_corpus.mm')\n",
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. How to create the TFIDF matrix (corpus) in gensim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "documents = [\"This is the first line\",\n",
    "             \"This is the second sentence\",\n",
    "             \"This third document\"]\n",
    "\n",
    "# Create the Dictionary and Corpus\n",
    "mydict = corpora.Dictionary([simple_preprocess(line) for line in documents])\n",
    "corpus = [mydict.doc2bow(simple_preprocess(line)) for line in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(4, 1), (7, 1), (8, 1)]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 first\n",
      "1 is\n",
      "2 line\n",
      "3 the\n",
      "4 this\n",
      "5 second\n",
      "6 sentence\n",
      "7 document\n",
      "8 third\n"
     ]
    }
   ],
   "source": [
    "for k,v in mydict.iteritems():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['first', 1], ['is', 1], ['line', 1], ['the', 1], ['this', 1]]\n",
      "[['is', 1], ['the', 1], ['this', 1], ['second', 1], ['sentence', 1]]\n",
      "[['this', 1], ['document', 1], ['third', 1]]\n"
     ]
    }
   ],
   "source": [
    "# Show the Word Weights in Corpus\n",
    "for doc in corpus:\n",
    "    print([[mydict[id], freq] for id, freq in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the TF-IDF model\n",
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.6633689723434505),\n",
       " (1, 0.24482975009584632),\n",
       " (2, 0.6633689723434505),\n",
       " (3, 0.24482975009584632)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[corpus][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.24482975009584632),\n",
       " (3, 0.24482975009584632),\n",
       " (5, 0.6633689723434505),\n",
       " (6, 0.6633689723434505)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[corpus][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 0.7071067811865476), (8, 0.7071067811865476)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[corpus][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['first', 0.66], ['is', 0.24], ['line', 0.66], ['the', 0.24]]\n",
      "[['is', 0.24], ['the', 0.24], ['second', 0.66], ['sentence', 0.66]]\n",
      "[['document', 0.71], ['third', 0.71]]\n"
     ]
    }
   ],
   "source": [
    "# Show the TF-IDF weights\n",
    "for doc in tfidf[corpus]:\n",
    "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The word \"THIS\" is ignored in all the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. How to use gensim downloader API to load datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "# api.load(\"glove-twitter-25\", return_path=True)\n",
    "# info = api.info()  # show info about available models/datasets\n",
    "# model = api.load(\"glove-twitter-25\")  # download the model and return as object ready for use\n",
    "# model.most_similar(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# api.load(name=\"glove-twitter-25\", return_path=True)\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-df2c333f5a59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Build the bigram models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphrases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPhrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Build the bigram models\n",
    "bigram = gensim.models.phrases.Phrases(dataset, min_count=3, threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n",
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n",
      "[ 0.00457867 -0.00061532  0.00060676 -0.00185214 -0.00076773  0.00287916\n",
      "  0.00216124 -0.00391873 -0.00013679 -0.00109255  0.00441408 -0.00095752\n",
      "  0.00185557  0.00260116 -0.000621   -0.00348139 -0.0047056   0.00105736\n",
      " -0.00461324 -0.00416416  0.00280248  0.00010413  0.00370574 -0.00053287\n",
      " -0.00123449  0.00264119  0.00072838  0.00108176  0.0042409   0.00353716\n",
      "  0.00084858 -0.00365485 -0.00397433  0.00051955  0.00356047  0.00246105\n",
      " -0.00183221 -0.00105863 -0.00212562  0.00462033 -0.00404473 -0.00097028\n",
      "  0.00294023 -0.00317279 -0.00129783  0.00401364  0.00081448  0.00430721\n",
      " -0.00139587 -0.00239847 -0.00468538 -0.00039326 -0.00370785  0.00220779\n",
      " -0.0013925   0.00148773 -0.00240456 -0.00217186 -0.00143837  0.00343744\n",
      "  0.00379231 -0.00218854  0.00123117 -0.00025535  0.00167947  0.00415604\n",
      " -0.00250504 -0.00273125  0.00324856  0.00094247 -0.00037369 -0.00227197\n",
      "  0.00171574 -0.00018387  0.0041366   0.00109504  0.00365612  0.00032269\n",
      "  0.00412002  0.00198101  0.00317242 -0.0010776  -0.0027922  -0.00467577\n",
      " -0.00378073 -0.00490096 -0.00412121  0.00126402 -0.00225596 -0.00305185\n",
      " -0.00232378 -0.00121612 -0.00016012  0.00117985 -0.00199385 -0.00339423\n",
      "  0.00461631 -0.00258574 -0.0040566   0.00425092]\n",
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahsood\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['final'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['first', 0.66], ['is', 0.24], ['line', 0.66], ['the', 0.24]]\n",
      "[['is', 0.24], ['the', 0.24], ['second', 0.66], ['sentence', 0.66]]\n",
      "[['document', 0.71], ['third', 0.71]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
